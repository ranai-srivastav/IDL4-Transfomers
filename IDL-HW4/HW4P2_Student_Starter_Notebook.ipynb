{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nAWXwbUxm-W"
   },
   "source": [
    "# HW4P2: Automatic Speech Recognition with an Encoder-Decoder Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJDfUJBexwxT"
   },
   "source": [
    "# Schedule:\n",
    "- Checkpoint Submission (DUE 21 November 2025 @ 11:59PM EST)\n",
    "- Kaggle Submission (DUE 5 December 2025 @ 11:59PM EST | Slack Deadline is 11 December 2025 @ 11:59PM EST)\n",
    "- Code Submission (DUE 7 December 2025 @ 11:59PM EST OR Day-of Slack submission)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgdtKjuqx0fj"
   },
   "source": [
    "## Requirement Acknowledgement\n",
    "Setting the below flag to True indicates full understanding and acceptance of the following:\n",
    "1. Slack days may ONLY be used on P2 FINAL (not checkpoint) submission. I.e. you may use slack days to submit final P2 kaggle scores (such as this one) later on the **SLACK KAGGLE COMPETITION** at the expense of your Slack days.\n",
    "2. The final autolab **code submission is due 48 hours after** the conclusion of the Kaggle Deadline (or, the same day as your final kaggle submission).\n",
    "3. We will require your kaggle username here, and then we will pull your official PRIVATE kaggle leaderboard score. This submission may result in slight variance in scores/code, but we will check for acceptable discrepancies. Any discrepancies related to modifying the submission code (at the bottom of the notebook) will result in an AIV.\n",
    "4. You are NOT allowed to use any code that will pre-load models (such as those from Hugging Face, etc.).\n",
    "   You MAY use models described by papers or articles, but you MUST implement them yourself through fundamental PyTorch operations (i.e. Linear, Conv2d, etc.).\n",
    "5. You are NOT allowed to use any external data/datasets at ANY point of this assignment.\n",
    "6. You may work with teammates to run ablations/experiments, BUT you must submit your OWN code and your OWN results.\n",
    "7. Failure to comply with the prior rules will be considered an Academic Integrity Violation (AIV).\n",
    "8. Late submissions MUST be submitted through the Slack Kaggle (see writeup for details). Any submissions made to the regular Kaggle after the original deadline will NOT be considered, no matter how many slack days remain for the student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTro6vOHx2qD"
   },
   "outputs": [],
   "source": [
    "ACKNOWLEDGED = True #TODO: Only set Acknowledged to True if you have read the above acknowlegements and agree to ALL of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksTZ5pAtXWXh"
   },
   "source": [
    "# Setup\n",
    "-  Follow the setup instructions based on your preferred environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdhQJTlgXWXk"
   },
   "source": [
    "## Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvlykMP_XWXl"
   },
   "source": [
    "One of our key goals in designing this assignment is to allow you to complete most of the preliminary implementation work locally.  \n",
    "We highly recommend that you **pass all tests locally** using the provided `hw4_data_subset` before moving to a GPU runtime.  \n",
    "To do this, simply:\n",
    "\n",
    "### Create a new conda environment\n",
    "```bash\n",
    "# Be sure to deactivate any active environments first\n",
    "conda create -n hw4 python=3.12.4\n",
    "```\n",
    "\n",
    "### Activate the conda environment\n",
    "```bash\n",
    "conda activate hw4\n",
    "```\n",
    "\n",
    "### Install the dependencies using the provided `requirements.txt`\n",
    "```bash\n",
    "pip install --no-cache-dir --ignore-installed -r requirements.txt\n",
    "```\n",
    "\n",
    "### Ensure that your notebook is in the same working directory as the `Handout`\n",
    "This can be achieved by:\n",
    "1. Physically moving the notebook into the handout directory.\n",
    "2. Changing the notebook‚Äôs current working directory to the handout directory using the os.chdir() function.\n",
    "\n",
    "### Open the notebook and select the newly created environment from the kernel selector.\n",
    "\n",
    "If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:\n",
    "```\n",
    ".\n",
    "‚îú‚îÄ‚îÄ README.md\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îú‚îÄ‚îÄ hw4lib/\n",
    "‚îú‚îÄ‚îÄ mytorch/\n",
    "‚îú‚îÄ‚îÄ tests/\n",
    "‚îî‚îÄ‚îÄ hw4_data_subset/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1K8ZJzQXWXm"
   },
   "source": [
    "## Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zBCpeYGXWXm"
   },
   "source": [
    "### Step 1: Get your handout\n",
    "- See writeup for recommended approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CEuLMiFXWXm"
   },
   "outputs": [],
   "source": [
    "# Example: My preferred approach\n",
    "import os\n",
    "# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)\n",
    "os.environ['GITHUB_TOKEN'] = \"your_github_token_here\"\n",
    "\n",
    "GITHUB_USERNAME = \"your_github_username_here\"\n",
    "REPO_NAME       = \"your_github_repo_name_here\"\n",
    "TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
    "repo_url        = f\"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "!git clone {repo_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Ov_XzmaXWXn"
   },
   "outputs": [],
   "source": [
    "# To pull latest changes (Must be in the repo dir, use pwd/ls to verify)\n",
    "!cd {REPO_NAME} && git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MczkQoLuXWXo"
   },
   "source": [
    "### Step 2: Install Dependencies\n",
    "- `NOTE`: Your runtime will be restarted to ensure all dependencies are updated.\n",
    "- `NOTE`: You will see a runtime crashed message, this was intentionally done. Simply move on to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eC4clcp2fenY"
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "%cd /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "GVyqU62HXWXo"
   },
   "outputs": [],
   "source": [
    "%pip install --no-deps -r IDL-HW4/requirements.txt\n",
    "import os\n",
    "os.kill(os.getpid(), 9) # NOTE: This will restart the your colab Python runtime (required)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVvxquxkfw2w"
   },
   "outputs": [],
   "source": [
    "!pip install transformers -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Dww5UXpXWXp"
   },
   "source": [
    "### Step 3: Obtain Data\n",
    "\n",
    "- `NOTE`: This process will automatically download and unzip data for both `HW4P1` and `HW4P2`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfPpo9nlXWXp"
   },
   "outputs": [],
   "source": [
    "!curl -L -o /content/f25-hw4-data.zip https://www.kaggle.com/api/v1/datasets/download/cmu11785/f25-11785-hw4-data\n",
    "!unzip -q -o /content/f25-hw4-data.zip -d /content/hw4_data\n",
    "!rm -rf /content/f25-hw4-data.zip\n",
    "!du -h --max-depth=2 /content/hw4_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RV797kwAXWXp"
   },
   "source": [
    "### Step 4: Move to Handout Directory\n",
    "You must be within the handout directory for the library imports to work!\n",
    "\n",
    "- `NOTE`: You may have to repeat running this command anytime you restart your runtime.\n",
    "- `NOTE`: You can do a `pwd` to check if you are in the right directory.\n",
    "- `NOTE`: The way it is setup currently, Your data directory should be one level up from your project directory. Keep this in mind when you are setting your `root` in the config file.\n",
    "\n",
    "If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:\n",
    "```\n",
    ".\n",
    "‚îú‚îÄ‚îÄ README.md\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îú‚îÄ‚îÄ hw4lib/\n",
    "‚îú‚îÄ‚îÄ mytorch/\n",
    "‚îú‚îÄ‚îÄ tests/\n",
    "‚îî‚îÄ‚îÄ hw4_data_subset/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQOe2uDxXWXp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('IDL-HW4')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uP0Aucc7XWXs"
   },
   "source": [
    "## PSC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJm_aSHvXWXs"
   },
   "source": [
    "### 1Ô∏è‚É£ **Step 1 Setting Up Your Environment on Bridges2**\n",
    "\n",
    "‚ùóÔ∏è‚ö†Ô∏è For this homework, we are **providing shared Datasets and a shared Conda environment** for the entire class.\n",
    "\n",
    "‚ùóÔ∏è‚ö†Ô∏è So for PSC users, **do not download the data yourself** and **do not need to manually install the packages**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Lf3a_SI7E8l"
   },
   "source": [
    "Follow these steps to set up the environment and start a Jupyter notebook on Bridges2:\n",
    "\n",
    "To run your notebook more efficiently on PSC, we need to use a **Jupyter Server** hosted on a compute node.\n",
    "\n",
    "You can use your prefered way of connecting to the Jupyter Server. Your options should be covered in the docs linked in post 558 @ piazza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyCQpcT07HQm"
   },
   "source": [
    "**The recommended way of connecting is:**\n",
    "\n",
    "#### **Connect in VSCode**\n",
    "SSH into Bridges2 and navigate to your **Jet directory** (`Jet/home/<your_psc_username>`). Upload your notebook there, and then connect to the Jupyter Server from that directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgivK7YU7J2g"
   },
   "source": [
    "#### **1. SSH into Bridges2**\n",
    "1ÔºâOpen VS Code and click on the `Extensions` icon in the left sidebar. Make sure the \"**Remote - SSH**\" extension is installed.\n",
    "\n",
    "2ÔºâOpen the command palette (**Shift+Command+P** on Mac, **Ctrl+Shift+P** on Windows). A search box will appear at the top center. Choose `\"Remote-SSH: Add New SSH Host\"`, then enter:\n",
    "\n",
    "```bash\n",
    "ssh <your_username>@bridges2.psc.edu #change <your_username> to your username\n",
    "```\n",
    "\n",
    "Next, choose `\"/Users/<your_username>/.ssh/config\"` as the config file. A dialog will appear in the bottom right saying \"Host Added\". Click `\"Connect\"`, and then enter your password.\n",
    "\n",
    "(Note: After adding the host once, you can later use `\"Remote-SSH: Connect to Host\"` and select \"bridges2.psc.edu\" from the list.)\n",
    "\n",
    "3ÔºâOnce connected, click `\"Explorer\"` in the left sidebar > \"Open Folder\", and navigate to your home directory under the project grant:\n",
    "```bash\n",
    "/jet/home/<your_username>  #change <your_username> to your username\n",
    "```\n",
    "\n",
    "4ÔºâYou can now drag your notebook files directly into the right-hand pane (your remote home directory), or upload them using `scp` into your folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EooKLI57OWj"
   },
   "source": [
    "> ‚ùóÔ∏è‚ö†Ô∏è The following steps should be executed in the **VSCode integrated terminal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euHM_evw7Qgz"
   },
   "source": [
    "#### **2. Navigate to Your Directory**\n",
    "Make sure to use this `/jet/home/<your_username>` as your working directory, since all subsequent operations (up to submission) are based on this path.\n",
    "```bash\n",
    "cd /jet/home/<your_username>  #change <your_username> to your username\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cAM9ezJ7S7o"
   },
   "source": [
    "#### **3. Request a Compute Node**\n",
    "```bash\n",
    "interact -p GPU-shared --gres=gpu:v100-32:1 -t 8:00:00 -A cis250019p\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-LgJDnc7UwG"
   },
   "source": [
    "#### **4. Load the Anaconda Module**\n",
    "```bash\n",
    "module load anaconda3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XhBfy767WjV"
   },
   "source": [
    "#### **5. Activate the provided HW4 Environment**\n",
    "```bash\n",
    "conda deactivate # First, deactivate any existing Conda environment\n",
    "######## [need to be updated] conda activate /ocean/projects/cis240101p/mzhang23/TA/HW4/envs/hw4_env && export PYTHONNOUSERSITE=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mwhhtkm97dNC"
   },
   "source": [
    "#### **6. Start Jupyter Notebook**\n",
    "Launch Jupyter Notebook:\n",
    "```bash\n",
    "jupyter notebook --no-browser --ip=0.0.0.0\n",
    "```\n",
    "\n",
    "Go to **Kernel** ‚Üí **Select Another Kernel** ‚Üí **Existing Jupyter Server**\n",
    "   Enter the URL of the Jupyter Server:```http://{hostname}:{port}/tree?token={token}```\n",
    "   \n",
    "   *(Usually, this URL appears in the terminal output after you run `jupyter notebook --no-browser --ip=0.0.0.0`, in a line like:  ‚ÄúJupyter Server is running at: http://...‚Äù)*\n",
    "\n",
    "   - eg: `http://v011.ib.bridges2.psc.edu:8888/tree?token=e4b302434e68990f28bc2b4ae8d216eb87eecb7090526249`\n",
    "\n",
    "> **Note**: Replace `{hostname}`, `{port}` and `{token}` with your actual values from the Jupyter output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHvhs7qP7ghQ"
   },
   "source": [
    "After launching the Jupyter notebook, you can run the cells directly inside the notebook ‚Äî no need to use the terminal for the remaining steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCemuI9_70PG"
   },
   "source": [
    "### 2Ô∏è‚É£ Step 2: Get Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaClGC468DoA"
   },
   "outputs": [],
   "source": [
    "#Make sure you are in your directory\n",
    "!pwd #should be /jet/home/<your_username>, if not, uncomment the following line and replace with your actual username:\n",
    "# %cd /jet/home/<your_username>\n",
    "#TODO: replace the \"<your_username>\" to yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "simQXvJoXWXs"
   },
   "outputs": [],
   "source": [
    "# Example: My preferred approach\n",
    "import os\n",
    "# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)\n",
    "os.environ['GITHUB_TOKEN'] = \"your_github_token_here\"\n",
    "\n",
    "GITHUB_USERNAME = \"your_github_username_here\"\n",
    "REPO_NAME       = \"your_github_repo_name_here\"\n",
    "TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
    "repo_url        = f\"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "!git clone {repo_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ibi0yUFXWXs"
   },
   "outputs": [],
   "source": [
    "# To pull latest changes (Must be in the repo dir, use pwd/ls to verify)\n",
    "!cd {REPO_NAME} && git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPU5y0G68brJ"
   },
   "source": [
    "#### **Move to Project Directory**\n",
    "- `NOTE`: You may have to repeat this on anytime you restart your runtime. You can do a `pwd` or `ls` to check if you are in the right directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtwYL_8q8Z5N"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('IDL-HW4')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVHYLiYb8exP"
   },
   "source": [
    "### 3Ô∏è‚É£ **Step 3: Set up Kaggle API Authentication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6QWOFH98h07"
   },
   "outputs": [],
   "source": [
    "# TODO: Use the same Kaggle code from HW3P2\n",
    "!mkdir /jet/home/<your_username>/.kaggle #TODO: replace the \"<your_username>\" to yours\n",
    "\n",
    "with open(\"/jet/home/<your_username>/.kaggle/kaggle.json\", \"w+\") as f: #TODO: replace the \"<your_username>\" to yours\n",
    "    f.write('{\"username\":\"<your_username>\",\"key\":\"<your_key>\"}')\n",
    "    # TODO: Put your kaggle username & key here\n",
    "\n",
    "!chmod 600 /jet/home/<your_username>/.kaggle/kaggle.json #TODO: replace the \"<your_username>\" to yours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHsBwCDn8kkq"
   },
   "source": [
    "### 4Ô∏è‚É£ **Step 4: Get Data**\n",
    "\n",
    "‚ùóÔ∏è‚ö†Ô∏è The data used in this assignment is **already stored in a shared, read-only folder, so you do not need to manually download anything**.\n",
    "\n",
    "Instead, just make sure to replace the dataset path in your notebook code with the correct path from the shared directory.\n",
    "\n",
    "You can run the following block to explore the shared directory structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFpjyS0z8nO_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = \"/ocean/projects/cis240101p/mzhang23/TA/HW4/hw4_data/hw4p2_data\" #Shared data path, do not need to change the username to yours\n",
    "print(\"Files in shared hw4p2 dataset:\", os.listdir(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xc9dAkrP8rof"
   },
   "outputs": [],
   "source": [
    "!apt-get install tree\n",
    "!tree -L 2 /ocean/projects/cis240101p/mzhang23/TA/HW4/hw4_data/hw4p2_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gv-iljarXWXw"
   },
   "source": [
    "# Imports\n",
    "- If your setup was done correctly, you should be able to run the following cell without any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "41s27pCAXWXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from hw4lib.data import (\n",
    "    H4Tokenizer,\n",
    "    ASRDataset,\n",
    "    verify_dataloader\n",
    ")\n",
    "from hw4lib.model import (\n",
    "    DecoderOnlyTransformer,\n",
    "    EncoderDecoderTransformer\n",
    ")\n",
    "from hw4lib.utils import (\n",
    "    create_scheduler,\n",
    "    create_optimizer,\n",
    "    plot_lr_schedule\n",
    ")\n",
    "from hw4lib.trainers import (\n",
    "    ASRTrainer,\n",
    "    ProgressiveTrainer\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import yaml\n",
    "import gc\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import json\n",
    "import wandb\n",
    "import pandas as pd\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5ey-Nd0XWXx"
   },
   "source": [
    "# Implementations\n",
    "- `NOTE`: All of these implementations have detailed specification, implementation details, and hints in their respective source files. Make sure to read all of them in their entirety to understand the implementation details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvQA7UchXWXx"
   },
   "source": [
    "## Dataset Implementation\n",
    "- Implement the `ASRDataset` class in `hw4lib/data/asr_dataset.py`.\n",
    "- You will have to implement parts of `__init__` and completely implement the `__len__`, `__getitem__` and `collate_fn` methods.\n",
    "- Run the cell below to check your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPZNGGJ9XWXx"
   },
   "outputs": [],
   "source": [
    "!python -m tests.test_dataset_asr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTKyK49VXWXx"
   },
   "source": [
    "## Model Implementations\n",
    "\n",
    "Overview:\n",
    "\n",
    "- Implement the `CrossAttentionLayer` class in `hw4lib/model/sublayers.py`.\n",
    "- Implement the `CrossAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.\n",
    "- Implement the `SelfAttentionEncoderLayer` class in `hw4lib/model/encoder_layers.py`. This will be mostly a copy-paste of the `SelfAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py` with one minor diffrence: it can attend to all positions in the input sequence.\n",
    "- Implement the `EncoderDecoderTransformer` class in `hw4lib/model/transformers.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POb3Agw6XWXy"
   },
   "source": [
    "### Transformer Sublayers\n",
    "- Now, Implement the `CrossAttentionLayer` class in `hw4lib/model/sublayers.py`.\n",
    "- `NOTE`: You should have already implemented the `SelfAttentionLayer`, and `FeedForwardLayer` classes in `hw4lib/model/sublayers.py`.\n",
    "- Run the cell below to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vYRTVdGXWXy"
   },
   "outputs": [],
   "source": [
    "!python -m tests.test_sublayer_crossattention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbIAaQ--XWXy"
   },
   "source": [
    "### Transformer Cross-Attention Decoder Layer\n",
    "- Implement the `CrossAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.\n",
    "- Then run the cell below to check your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4Ke3vAVXWXy"
   },
   "outputs": [],
   "source": [
    "!python -m tests.test_decoderlayer_crossattention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28snypxrXWXy"
   },
   "source": [
    "### Transformer Self-Attention Encoder Layer\n",
    "- Implement the `SelfAttentionEncoderLayer` class in `hw4lib/model/encoder_layers.py`.\n",
    "- Then run the cell below to check your implementation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_bRj56mXWXy"
   },
   "outputs": [],
   "source": [
    "!python -m tests.test_encoderlayer_selfattention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1b9Co8HXWXy"
   },
   "source": [
    "### Encoder-Decoder Transformer\n",
    "\n",
    "- Implement the  `EncoderDecoderTransformer` class in `hw4lib/model/transformers.py`.\n",
    "- Then run the cell below to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ACVZWXrXWXy"
   },
   "outputs": [],
   "source": [
    "!python -m tests.test_transformer_encoder_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxMaLdHdXWXy"
   },
   "source": [
    "## Decoding Implementation\n",
    "- We highly recommend you to implement the `generate_beam` method of the `SequenceGenerator` class in `hw4lib/decoding/sequence_generator.py`.\n",
    "- Then run the cell below to check your implementation.\n",
    "- `NOTE`: This is an optional but highly recommended task for `HW4P2` to ease the journey to high cutoffs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnAQO_VHXWXy"
   },
   "outputs": [],
   "source": [
    "!python -m tests.test_decoding --mode beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ze6Ufzt2XWXy"
   },
   "source": [
    "## Trainer Implementation\n",
    "You will have to do some minor in-filling for the `ASRTrainer` class in `hw4lib/trainers/asr_trainer.py` before you can use it.\n",
    "- Fill in the `TODO`s in the `__init__`.\n",
    "- Fill in the `TODO`s in the `_train_epoch`.\n",
    "- Fill in the `TODO`s in the `recognize` method.\n",
    "- Fill in the `TODO`s in the `_validate_epoch`.\n",
    "- Fill in the `TODO`s in the `train` method.\n",
    "- Fill in the `TODO`s in the `evaluate` method.\n",
    "\n",
    "`WARNING`: There are no test's for this. Implement carefully!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8CtOMbVXWXz"
   },
   "source": [
    "# Experiments\n",
    "From this point onwards you may want to switch to a `GPU` runtime.\n",
    "- `OBJECTIVE`: Optimize your model for `CER` on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-P4qGdjXWXz"
   },
   "source": [
    "## Config\n",
    "- You can use the `config.yaml` file to set your config for your ablation study.\n",
    "\n",
    "---\n",
    "### Notes:\n",
    "\n",
    "- Set `tokenization: token_type:` to specify your desired tokenization strategy\n",
    "- You will need to set the root path to your `hw4p1_data` folder in `data: root:`. This will depend on your setup. For eg. if you are following out setup instruction:\n",
    "  - `PSC`: `\"/local/hw4_data/hw4p1_data\"`\n",
    "  - `Colab:`: `\"/content/hw4_data/hw4p1_data\"`\n",
    "- There's extra configurations in the `optimizer` section which will only be relevant if you decide to use the `create_optimizer` function we've provided in `hw4lib/utils/create_optimizer.py`.\n",
    "- `BE CAREFUL` while setting numeric values. Eg. `1e-4` will get serialized to a `str` while `1.0e-4` gets serialized to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pkFcCs6VXWXz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "Name                      : \"Ranai Srivastav\"\n",
    "\n",
    "###### Tokenization ------------------------------------------------------------\n",
    "tokenization:\n",
    "  token_type                : \"5k\"       # [char, 1k, 5k, 10k]\n",
    "  token_map :\n",
    "      'char': 'hw4lib/data/tokenizer_jsons/tokenizer_char.json'\n",
    "      '1k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_1000.json'\n",
    "      '5k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_5000.json'\n",
    "      '10k' : 'hw4lib/data/tokenizer_jsons/tokenizer_10000.json'\n",
    "\n",
    "###### Dataset -----------------------------------------------------------------\n",
    "data:\n",
    "  root                 : \"hw4_data_subset/hw4p2_data\"  # TODO: Set the root path of your data\n",
    "  train_partition      : \"train-clean-100\"  # paired text-speech for ASR pre-training\n",
    "  val_partition        : \"dev-clean\"        # paired text-speech for ASR pre-training\n",
    "  test_partition       : \"test-clean\"       # paired text-speech for ASR pre-training\n",
    "  subset               : 1.0                # Load a subset of the data (for debugging, testing, etc\n",
    "  batch_size           : 2           #\n",
    "  NUM_WORKERS          : 2            # Set to 0 for CPU\n",
    "  norm                 : 'global_mvn' # ['global_mvn', 'cepstral', 'none']\n",
    "  num_feats            : 80\n",
    "\n",
    "  ###### SpecAugment ---------------------------------------------------------------\n",
    "  specaug                   : False  # Set to True if you want to use SpecAugment\n",
    "  specaug_conf:\n",
    "    apply_freq_mask         : True\n",
    "    freq_mask_width_range   : 5\n",
    "    num_freq_mask           : 2\n",
    "    apply_time_mask         : True\n",
    "    time_mask_width_range   : 40\n",
    "    num_time_mask           : 2\n",
    "\n",
    "###### Network Specs -------------------------------------------------------------\n",
    "model: # Encoder-Decoder Transformer (HW4P2)\n",
    "  # Speech embedding parameters\n",
    "  input_dim: 80              # Speech feature dimension\n",
    "  time_reduction: 2          # Time dimension downsampling factor\n",
    "  reduction_method: 'conv'   # The source_embedding reduction method ['lstm', 'conv', 'both']\n",
    "\n",
    "  # Architecture parameters\n",
    "  d_model: 256          # Model dimension\n",
    "  num_encoder_layers: 6  # Number of encoder layers\n",
    "  num_decoder_layers: 6  # Number of decoder layers\n",
    "  num_encoder_heads: 4   # Number of encoder attention heads\n",
    "  num_decoder_heads: 4   # Number of decoder attention heads\n",
    "  d_ff_encoder: 1024     # Feed-forward dimension for encoder\n",
    "  d_ff_decoder: 1024     # Feed-forward dimension for decoder\n",
    "  skip_encoder_pe: False # Whether to skip positional encoding for encoder\n",
    "  skip_decoder_pe: False # Whether to skip positional encoding for decoder\n",
    "\n",
    "  # Common parameters\n",
    "  dropout: 0.0          # Dropout rate\n",
    "  layer_drop_rate: 0.0  # Layer dropout rate\n",
    "  weight_tying: False   # Whether to use weight tying\n",
    "\n",
    "###### Common Training Parameters ------------------------------------------------\n",
    "training:\n",
    "  use_wandb                   : True   # Toggle wandb logging\n",
    "  wandb_run_id                : \"none\" # \"none\" or \"run_id\"\n",
    "  resume                      : True   # Resume an existing run (run_id != 'none')\n",
    "  gradient_accumulation_steps : 1\n",
    "  wandb_project               : \"HW4P2-ranais\" # wandb project to log to\n",
    "\n",
    "###### Loss ----------------------------------------------------------------------\n",
    "loss: # Just good ol' CrossEntropy\n",
    "  label_smoothing: 0.05\n",
    "  ctc_weight: 0.2\n",
    "\n",
    "###### Optimizer -----------------------------------------------------------------\n",
    "optimizer:\n",
    "  name: \"adamw\" # Options: sgd, adam, adamw\n",
    "  lr: 0.0004    # Base learning rate\n",
    "\n",
    "  # Common parameters\n",
    "  weight_decay: 0.000001\n",
    "\n",
    "  # Parameter groups\n",
    "  # You can add more param groups as you want and set their learning rates and patterns\n",
    "  param_groups:\n",
    "    - name: self_attn\n",
    "      patterns: []  # Will match all parameters containing \"ffn\" and set their learning rate to 0.0002\n",
    "      lr: 0.0002    # LR for self_attn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "\n",
    "    - name: ffn\n",
    "      patterns: [] # Will match all parameters containing \"ffn\" and set their learning rate to 0.0002\n",
    "      lr: 0.0002   # LR for ffn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "\n",
    "  # Layer-wise learning rates\n",
    "  layer_decay:\n",
    "    enabled: False\n",
    "    decay_rate: 0.75\n",
    "\n",
    "  # SGD specific parameters\n",
    "  sgd:\n",
    "    momentum: 0.9\n",
    "    nesterov: True\n",
    "    dampening: 0\n",
    "\n",
    "  # Adam specific parameters\n",
    "  adam:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "  # AdamW specific parameters\n",
    "  adamw:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "###### Scheduler -----------------------------------------------------------------\n",
    "scheduler:\n",
    "  name: \"cosine\"  # Options: reduce_lr, cosine, cosine_warm\n",
    "\n",
    "  # ReduceLROnPlateau specific parameters\n",
    "  reduce_lr:\n",
    "    mode: \"min\"  # Options: min, max\n",
    "    factor: 0.1  # Factor to reduce learning rate by\n",
    "    patience: 10  # Number of epochs with no improvement after which LR will be reduced\n",
    "    threshold: 0.0001  # Threshold for measuring the new optimum\n",
    "    threshold_mode: \"rel\"  # Options: rel, abs\n",
    "    cooldown: 0  # Number of epochs to wait before resuming normal operation\n",
    "    min_lr: 0.0000001  # Minimum learning rate\n",
    "    eps: 1e-8  # Minimal decay applied to lr\n",
    "\n",
    "  # CosineAnnealingLR specific parameters\n",
    "  cosine:\n",
    "    T_max: 15  # Maximum number of iterations\n",
    "    eta_min: 0.0000001  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # CosineAnnealingWarmRestarts specific parameters\n",
    "  cosine_warm:\n",
    "    T_0: 10    # Number of iterations for the first restart\n",
    "    T_mult: 10 # Factor increasing T_i after each restart\n",
    "    eta_min: 0.0000001  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # Warmup parameters (can be used with any scheduler)\n",
    "  warmup:\n",
    "    enabled: True\n",
    "    type: \"exponential\"  # Options: linear, exponential\n",
    "    epochs: 5\n",
    "    start_factor: 0.1\n",
    "    end_factor: 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ycLt56sWXWXz"
   },
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lq8X3BcuXWXz"
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gstAD2BGXWXz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                          Tokenizer Configuration (5k)                          \n",
      "--------------------------------------------------------------------------------\n",
      "Vocabulary size:     5000\n",
      "\n",
      "Special Tokens:\n",
      "PAD:              0\n",
      "UNK:              1\n",
      "MASK:             2\n",
      "SOS:              3\n",
      "EOS:              4\n",
      "BLANK:            5\n",
      "\n",
      "Validation Example:\n",
      "--------------------------------------------------------------------------------\n",
      "Input text:  [SOS]HI DEEP LEARNERS[EOS]\n",
      "Tokens:      ['[SOS]', 'H', 'I', 'ƒ†DEEP', 'ƒ†LEARN', 'ERS', '[EOS]']\n",
      "Token IDs:   [3, 14, 15, 1169, 2545, 214, 4]\n",
      "Decoded:     [SOS]HI DEEP LEARNERS[EOS]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "Tokenizer = H4Tokenizer(\n",
    "    token_map  = config['tokenization']['token_map'],\n",
    "    token_type = config['tokenization']['token_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hCLVwnaXWXz"
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DwmZydebXWXz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for train-clean-100 partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 194.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global stats computed from training set.\n",
      "Loading data for dev-clean partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 827.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for test-clean partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 1695.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = ASRDataset(\n",
    "    partition=config['data']['train_partition'],\n",
    "    config=config['data'],\n",
    "    tokenizer=Tokenizer,\n",
    "    isTrainPartition=True,\n",
    "    global_stats=None  # Will compute stats from training data\n",
    ")\n",
    "\n",
    "# TODO: Get the computed global stats from training set\n",
    "global_stats = None\n",
    "if config['data']['norm'] == 'global_mvn':\n",
    "    global_stats = (train_dataset.global_mean, train_dataset.global_std)\n",
    "    print(f\"Global stats computed from training set.\")\n",
    "\n",
    "val_dataset = ASRDataset(\n",
    "    partition=config['data']['val_partition'],\n",
    "    config=config['data'],\n",
    "    tokenizer=Tokenizer,\n",
    "    isTrainPartition=False,\n",
    "    global_stats=global_stats\n",
    ")\n",
    "\n",
    "test_dataset = ASRDataset(\n",
    "    partition=config['data']['test_partition'],\n",
    "    config=config['data'],\n",
    "    tokenizer=Tokenizer,\n",
    "    isTrainPartition=False,\n",
    "    global_stats=global_stats\n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAFMrdMLXWXz"
   },
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SGRynmFPXWXz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader    = DataLoader(\n",
    "    dataset     = train_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = True,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = train_dataset.collate_fn\n",
    ")\n",
    "\n",
    "val_loader      = DataLoader(\n",
    "    dataset     = val_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = False,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = val_dataset.collate_fn\n",
    ")\n",
    "\n",
    "test_loader     = DataLoader(\n",
    "    dataset     = test_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = False,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = test_dataset.collate_fn\n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YILqrSuXWX0"
   },
   "source": [
    "### Dataloader Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awodg0EWXWX0"
   },
   "outputs": [],
   "source": [
    "verify_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPNijuHBXWX0"
   },
   "outputs": [],
   "source": [
    "verify_dataloader(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYjQOBj2XWX0"
   },
   "outputs": [],
   "source": [
    "verify_dataloader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8_T2fH3XWX0"
   },
   "source": [
    "## Calculate Max Lengths\n",
    "Calculating the maximum transcript length across your dataset is a crucial step when working with certain transformer models.\n",
    "-  We'll use sinusoidal positional encodings that must be precomputed up to a fixed maximum length.\n",
    "- This maximum length is a hyperparameter that determines:\n",
    "  - How long of a sequence your model can process\n",
    "  - The size of your positional encoding matrix\n",
    "  - Memory requirements during training and inference\n",
    "- `Requirements`: For this assignment, ensure your positional encodings can accommodate at least the longest sequence in your dataset to prevent truncation. However, you can set this value higher if you anticipate using your languagemodel to work with longer sequences in future tasks (hint: this might be useful for P2! üòâ).\n",
    "- `NOTE`: We'll be using the same positional encoding matrix for all sequences in your dataset. Take this into account when setting your maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "guY_wgFjXWX0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Max Feature Length             : 2047\n",
      "Max Transcript Length          : 78\n",
      "Overall Max Length             : 2047\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_feat_len       = max(train_dataset.feat_max_len, val_dataset.feat_max_len, test_dataset.feat_max_len)\n",
    "max_transcript_len = max(train_dataset.text_max_len, val_dataset.text_max_len, test_dataset.text_max_len)\n",
    "max_len            = max(max_feat_len, max_transcript_len)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Max Feature Length':<30} : {max_feat_len}\")\n",
    "print(f\"{'Max Transcript Length':<30} : {max_transcript_len}\")\n",
    "print(f\"{'Overall Max Length':<30} : {max_len}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmryM_AHXWX0"
   },
   "source": [
    "## Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5fEQ0Ns_XWX0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mranais\u001b[0m (\u001b[33mmrsd-smores\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXcjkHQ_XWX0"
   },
   "source": [
    "## Training\n",
    "\n",
    "Every time you run the trainer, it will create a new directory in the `expts` folder with the following structure:\n",
    "```\n",
    "expts/\n",
    "    ‚îî‚îÄ‚îÄ {run_name}/\n",
    "        ‚îú‚îÄ‚îÄ config.yaml\n",
    "        ‚îú‚îÄ‚îÄ model_arch.txt\n",
    "        ‚îú‚îÄ‚îÄ checkpoints/\n",
    "        ‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-best-metric-model.pth\n",
    "        ‚îÇ   ‚îî‚îÄ‚îÄ checkpoint-last-epoch-model.pth\n",
    "        ‚îú‚îÄ‚îÄ attn/\n",
    "        ‚îÇ   ‚îî‚îÄ‚îÄ {attention visualizations}\n",
    "        ‚îî‚îÄ‚îÄ text/\n",
    "            ‚îî‚îÄ‚îÄ {generated text outputs}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DS7fwKk5XWX0"
   },
   "source": [
    "### Training Strategy 1: Cold-Start Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-y9NyN4XWX0"
   },
   "source": [
    "#### Model Load (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CTSjyngVXWX0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "EncoderDecoderTransformer                     [2, 60, 5000]             --\n",
      "‚îú‚îÄSpeechEmbedding: 1-1                        [2, 870, 256]             --\n",
      "‚îÇ    ‚îî‚îÄConv2DSubsampling: 2-1                 [2, 870, 256]             --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-1                   [2, 256, 870, 76]         592,640\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄLinear: 3-2                       [2, 870, 256]             4,980,992\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-3                      [2, 870, 256]             --\n",
      "‚îú‚îÄPositionalEncoding: 1-2                     [2, 870, 256]             --\n",
      "‚îú‚îÄDropout: 1-3                                [2, 870, 256]             --\n",
      "‚îú‚îÄModuleList: 1-4                             --                        --\n",
      "‚îÇ    ‚îî‚îÄSelfAttentionEncoderLayer: 2-2         [2, 870, 256]             --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄSelfAttentionLayer: 3-4           [2, 870, 256]             263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄFeedForwardLayer: 3-5             [2, 870, 256]             526,080\n",
      "‚îÇ    ‚îî‚îÄSelfAttentionEncoderLayer: 2-3         [2, 870, 256]             --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄSelfAttentionLayer: 3-6           [2, 870, 256]             263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄFeedForwardLayer: 3-7             [2, 870, 256]             526,080\n",
      "‚îÇ    ‚îî‚îÄSelfAttentionEncoderLayer: 2-4         [2, 870, 256]             --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄSelfAttentionLayer: 3-8           [2, 870, 256]             263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄFeedForwardLayer: 3-9             [2, 870, 256]             526,080\n",
      "‚îÇ    ‚îî‚îÄSelfAttentionEncoderLayer: 2-5         [2, 870, 256]             --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄSelfAttentionLayer: 3-10          [2, 870, 256]             263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄFeedForwardLayer: 3-11            [2, 870, 256]             526,080\n",
      "‚îÇ    ‚îî‚îÄSelfAttentionEncoderLayer: 2-6         [2, 870, 256]             --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄSelfAttentionLayer: 3-12          [2, 870, 256]             263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄFeedForwardLayer: 3-13            [2, 870, 256]             526,080\n",
      "‚îÇ    ‚îî‚îÄSelfAttentionEncoderLayer: 2-7         [2, 870, 256]             --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄSelfAttentionLayer: 3-14          [2, 870, 256]             263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄFeedForwardLayer: 3-15            [2, 870, 256]             526,080\n",
      "‚îú‚îÄLayerNorm: 1-5                              [2, 870, 256]             512\n",
      "‚îú‚îÄSequential: 1-6                             [2, 870, 5000]            --\n",
      "‚îÇ    ‚îî‚îÄLinear: 2-8                            [2, 870, 5000]            1,285,000\n",
      "‚îÇ    ‚îî‚îÄLogSoftmax: 2-9                        [2, 870, 5000]            --\n",
      "‚îú‚îÄEmbedding: 1-7                              [2, 60, 256]              1,280,000\n",
      "‚îú‚îÄPositionalEncoding: 1-8                     [2, 60, 256]              --\n",
      "‚îú‚îÄDropout: 1-9                                [2, 60, 256]              --\n",
      "‚îú‚îÄModuleList: 1-10                            --                        --\n",
      "‚îÇ    ‚îî‚îÄCrossAttentionDecoderLayer: 2-10       [2, 60, 256]              --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄSelfAttentionLayer: 3-16          [2, 60, 256]              263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄCrossAttentionLayer: 3-17         [2, 60, 256]              263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄFeedForwardLayer: 3-18            [2, 60, 256]              526,080\n",
      "‚îÇ    ‚îî‚îÄCrossAttentionDecoderLayer: 2-11       [2, 60, 256]              --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄSelfAttentionLayer: 3-19          [2, 60, 256]              263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄCrossAttentionLayer: 3-20         [2, 60, 256]              263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄFeedForwardLayer: 3-21            [2, 60, 256]              526,080\n",
      "‚îÇ    ‚îî‚îÄCrossAttentionDecoderLayer: 2-12       [2, 60, 256]              --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄSelfAttentionLayer: 3-22          [2, 60, 256]              263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄCrossAttentionLayer: 3-23         [2, 60, 256]              263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄFeedForwardLayer: 3-24            [2, 60, 256]              526,080\n",
      "‚îÇ    ‚îî‚îÄCrossAttentionDecoderLayer: 2-13       [2, 60, 256]              --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄSelfAttentionLayer: 3-25          [2, 60, 256]              263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄCrossAttentionLayer: 3-26         [2, 60, 256]              263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄFeedForwardLayer: 3-27            [2, 60, 256]              526,080\n",
      "‚îÇ    ‚îî‚îÄCrossAttentionDecoderLayer: 2-14       [2, 60, 256]              --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄSelfAttentionLayer: 3-28          [2, 60, 256]              263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄCrossAttentionLayer: 3-29         [2, 60, 256]              263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄFeedForwardLayer: 3-30            [2, 60, 256]              526,080\n",
      "‚îÇ    ‚îî‚îÄCrossAttentionDecoderLayer: 2-15       [2, 60, 256]              --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄSelfAttentionLayer: 3-31          [2, 60, 256]              263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄCrossAttentionLayer: 3-32         [2, 60, 256]              263,680\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄFeedForwardLayer: 3-33            [2, 60, 256]              526,080\n",
      "‚îú‚îÄLayerNorm: 1-11                             [2, 60, 256]              512\n",
      "‚îú‚îÄLinear: 1-12                                [2, 60, 5000]             1,285,000\n",
      "===============================================================================================\n",
      "Total params: 20,483,856\n",
      "Trainable params: 20,483,856\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 78.41\n",
      "===============================================================================================\n",
      "Input size (MB): 1.12\n",
      "Forward/backward pass size (MB): 814.29\n",
      "Params size (MB): 62.99\n",
      "Estimated Total Size (MB): 878.39\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_config = config['model'].copy()\n",
    "model_config.update({\n",
    "    'max_len': max_len,\n",
    "    'num_classes': Tokenizer.vocab_size\n",
    "})\n",
    "\n",
    "model = EncoderDecoderTransformer(**model_config).to(device=device).float()\n",
    "\n",
    "# Get some inputs from the train dataloader\n",
    "for batch in train_loader:\n",
    "    padded_feats, padded_shifted, padded_golden, feat_lengths, transcript_lengths = batch\n",
    "    padded_feats = padded_feats.to(device=device) \n",
    "    padded_shifted = padded_shifted.to(device=device)\n",
    "    padded_golden = padded_golden.to(device=device)\n",
    "    feat_lengths = feat_lengths.to(device=device)\n",
    "    transcript_lengths = transcript_lengths.to(device=device)\n",
    "    break\n",
    "\n",
    "total_param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "assert total_param < 30_000_000, f\"Total trainable parameters ({total_param}) exceeds 30 million.\"\n",
    "\n",
    "model_stats = summary(model, input_data=[padded_feats, padded_shifted, feat_lengths, transcript_lengths])\n",
    "print(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FN6wc57eXWX1"
   },
   "source": [
    "#### Initialize Trainer\n",
    "\n",
    "If you need to reload the model from a checkpoint, you can do so by calling the `load_checkpoint` method.\n",
    "\n",
    "```python\n",
    "checkpoint_path = \"path/to/checkpoint.pth\"\n",
    "trainer.load_checkpoint(checkpoint_path)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1eg-NsHzXWX1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ranai/MRSD/idl/hw4/IDL4-Transfomers/IDL-HW4/wandb/run-20251209_030217-mtl0krqe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mrsd-smores/HW4P2-ranais/runs/mtl0krqe' target=\"_blank\">HW4P2_ablation1</a></strong> to <a href='https://wandb.ai/mrsd-smores/HW4P2-ranais' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mrsd-smores/HW4P2-ranais' target=\"_blank\">https://wandb.ai/mrsd-smores/HW4P2-ranais</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mrsd-smores/HW4P2-ranais/runs/mtl0krqe' target=\"_blank\">https://wandb.ai/mrsd-smores/HW4P2-ranais/runs/mtl0krqe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = ASRTrainer(\n",
    "    model=model,\n",
    "    tokenizer=Tokenizer,\n",
    "    config=config,\n",
    "    run_name=\"HW4P2_ablation1\", # TODO: Change to your desired wandb run name\n",
    "    config_file=\"config.yaml\",\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiQSYD7jXWX1"
   },
   "source": [
    "### Setup Optimizer and Scheduler\n",
    "\n",
    "You can set your own optimizer and scheduler by setting the class members in the `LMTrainer` class.\n",
    "Eg:\n",
    "```python\n",
    "trainer.optimizer = optim.AdamW(model.parameters(), lr=config['optimizer']['lr'], weight_decay=config['optimizer']['weight_decay'])\n",
    "trainer.scheduler = optim.lr_scheduler.CosineAnnealingLR(trainer.optimizer, T_max=config['training']['epochs'])\n",
    "```\n",
    "\n",
    "We also provide a utility function to create your own optimizer and scheduler with the congig and some extra bells and whistles. You are free to use it or not. Do read their code and documentation to understand how it works (`hw4lib/utils/*`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_IQUFhmXWX1"
   },
   "source": [
    "#### Setting up the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "F1mMBVjrXWX1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Configuring Optimizer:\n",
      "‚îú‚îÄ‚îÄ Type: ADAMW\n",
      "‚îú‚îÄ‚îÄ Base LR: 0.0004\n",
      "‚îú‚îÄ‚îÄ Weight Decay: 1e-06\n",
      "‚îú‚îÄ‚îÄ Parameter Groups:\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ Group: self_attn\n",
      "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LR: 0.0002\n",
      "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Patterns: []\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ Group: ffn\n",
      "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LR: 0.0002\n",
      "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Patterns: []\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ Default Group (unmatched parameters)\n",
      "‚îî‚îÄ‚îÄ AdamW Specific:\n",
      "    ‚îú‚îÄ‚îÄ Betas: [0.9, 0.999]\n",
      "    ‚îú‚îÄ‚îÄ Epsilon: 1e-08\n",
      "    ‚îî‚îÄ‚îÄ AMSGrad: False\n"
     ]
    }
   ],
   "source": [
    "trainer.optimizer = create_optimizer(\n",
    "    model=model,\n",
    "    opt_config=config['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_h4o2qWEXWX1"
   },
   "source": [
    "#### Creating a test scheduler and plotting the learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ilPlvS6EXWX1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Configuring Learning Rate Scheduler:\n",
      "‚îú‚îÄ‚îÄ Type: COSINE\n",
      "‚îú‚îÄ‚îÄ Cosine Annealing Settings:\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ T_max: 15 epochs (210 steps)\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ Min LR: 1e-07\n",
      "‚îú‚îÄ‚îÄ Warmup Settings:\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ Duration: 5 epochs (70 steps)\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ Start Factor: 0.1\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ End Factor: 1.0\n",
      "Warning: Only showing 5 out of 195 parameter groups for clarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ranai/miniconda3/envs/idl/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_scheduler = create_scheduler(\n",
    "    optimizer=trainer.optimizer,\n",
    "    scheduler_config=config['scheduler'],\n",
    "    train_loader=train_loader,\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps'])\n",
    "\n",
    "plot_lr_schedule(\n",
    "    scheduler=test_scheduler,\n",
    "    num_epochs=20,\n",
    "    train_loader=train_loader,\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1qQK403XWX1"
   },
   "source": [
    "#### Setting up the scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "H3-MTweSXWX2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Configuring Learning Rate Scheduler:\n",
      "‚îú‚îÄ‚îÄ Type: COSINE\n",
      "‚îú‚îÄ‚îÄ Cosine Annealing Settings:\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ T_max: 15 epochs (210 steps)\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ Min LR: 1e-07\n",
      "‚îú‚îÄ‚îÄ Warmup Settings:\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ Duration: 5 epochs (70 steps)\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ Start Factor: 0.1\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ End Factor: 1.0\n"
     ]
    }
   ],
   "source": [
    "trainer.scheduler = create_scheduler(\n",
    "    optimizer=trainer.optimizer,\n",
    "    scheduler_config=config['scheduler'],\n",
    "    train_loader=train_loader,\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9y3ASKqWXWX2"
   },
   "source": [
    "#### Train\n",
    "- Set your epochs and start training!\n",
    "- `NOTE`: A `scheduler` gets initialized in this call based on the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bKCzg3U8XWX2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training ASR]:   0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1959, 80])\n",
      "torch.Size([2, 72])\n",
      "torch.Size([2, 72])\n",
      "torch.Size([2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input_lengths must be of size batch_size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MRSD/idl/hw4/IDL4-Transfomers/IDL-HW4/hw4lib/trainers/asr_trainer.py:267\u001b[0m, in \u001b[0;36mASRTrainer.train\u001b[0;34m(self, train_dataloader, val_dataloader, epochs)\u001b[0m\n\u001b[1;32m    262\u001b[0m best_val_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch \u001b[38;5;241m+\u001b[39m epochs):\n\u001b[1;32m    265\u001b[0m \n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# TODO: Train for one epoch\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     train_metrics, train_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;66;03m# TODO: Validate\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     val_metrics, val_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_epoch(val_dataloader)\n",
      "File \u001b[0;32m~/MRSD/idl/hw4/IDL4-Transfomers/IDL-HW4/hw4lib/trainers/asr_trainer.py:132\u001b[0m, in \u001b[0;36mASRTrainer._train_epoch\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# TODO: Calculate CTC loss if needed\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctc_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 132\u001b[0m     ctc_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_criterion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctc_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_probs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# T x B x C\u001b[39;49;00m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtargets_golden\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctc_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlengths\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtranscript_lengths\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     loss \u001b[38;5;241m=\u001b[39m ce_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctc_weight \u001b[38;5;241m*\u001b[39m ctc_loss\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/idl/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/idl/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/idl/lib/python3.10/site-packages/torch/nn/modules/loss.py:1984\u001b[0m, in \u001b[0;36mCTCLoss.forward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   1977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1978\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1979\u001b[0m     log_probs: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1982\u001b[0m     target_lengths: Tensor,\n\u001b[1;32m   1983\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1984\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1987\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1989\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1990\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1991\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_infinity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1992\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/idl/lib/python3.10/site-packages/torch/nn/functional.py:3079\u001b[0m, in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[1;32m   3068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3069\u001b[0m         ctc_loss,\n\u001b[1;32m   3070\u001b[0m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3077\u001b[0m         zero_infinity\u001b[38;5;241m=\u001b[39mzero_infinity,\n\u001b[1;32m   3078\u001b[0m     )\n\u001b[0;32m-> 3079\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3082\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3085\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mzero_infinity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3087\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input_lengths must be of size batch_size"
     ]
    }
   ],
   "source": [
    "trainer.train(train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9o7QclnXWX2"
   },
   "source": [
    "#### Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmtZ3E46N0Kt"
   },
   "outputs": [],
   "source": [
    "# Define the recognition config: Greedy search\n",
    "recognition_config = {\n",
    "    'num_batches': None,\n",
    "    'temperature': 1.0,\n",
    "    'repeat_penalty': 1.0,\n",
    "    'lm_weight': None,\n",
    "    'lm_model': None,\n",
    "    'beam_width': 6, # Beam width of 1 reverts to greedy\n",
    "}\n",
    "\n",
    "# Recognize with the shallow fusion config\n",
    "config_name = \"test\"\n",
    "print(f\"Evaluating with {config_name} config\")\n",
    "results = trainer.recognize(test_loader, recognition_config, config_name=config_name, max_length=max_transcript_len)\n",
    "\n",
    "\n",
    "# Calculate metrics on full batch\n",
    "generated = [r['generated'] for r in results]\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        'id': range(len(generated)),\n",
    "        'transcription': generated\n",
    "    }\n",
    ")\n",
    "\n",
    "# Cleanup (Will end wandb run)\n",
    "trainer.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Srfle5jUeGYA"
   },
   "source": [
    "## Submit to Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n-yE2ca2Mk4"
   },
   "source": [
    "### Authenticate Kaggle\n",
    "In order to use the Kaggle‚Äôs public API, you must first authenticate using an API token. Go to the 'Account' tab of your user profile and select 'Create New Token'. This will trigger the download of kaggle.json, a file containing your API credentials.\n",
    "- `TODO`: Set your kaggle username and api key here based on the API credentials listed in the kaggle.json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYzw-CoA2Mk4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KAGGLE_USERNAME\"] = \"your_kaggle_username_here\"\n",
    "os.environ[\"KAGGLE_KEY\"] = \"your_kaggle_api_key_here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFRZqdazOfZt"
   },
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bz35_aAg2Mk4"
   },
   "source": [
    "### Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqWwBD6kO0Zf"
   },
   "outputs": [],
   "source": [
    "results_df.to_csv(\"results.csv\", index=False)\n",
    "!kaggle competitions submit -c 11-785-hw-4-p-2-automatic-speech-recognition-f-25 -f results.csv -m \"My Submission\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vj2ma1oL1ZAN"
   },
   "source": [
    "#### TODO: Generate a model_metadata.json file to save your model's data (due 48 hours after Kaggle submission deadline OR the day of slack submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZH8fnVP1Zai"
   },
   "outputs": [],
   "source": [
    "import json, os, sys, torch, datetime\n",
    "################################\n",
    "# TODO: Keep the model_metadata.json\n",
    "# file safe for submission ater.\n",
    "################################\n",
    "def is_colab():\n",
    "    return \"google.colab\" in sys.modules and \"COLAB_GPU\" in os.environ\n",
    "\n",
    "def is_kaggle():\n",
    "    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ or \"KAGGLE_URL_BASE\" in os.environ\n",
    "\n",
    "def generate_model_submission_file(model):\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    json_filename = f\"model_metadata_{timestamp}.json\"\n",
    "\n",
    "    # Create JSON with parameter count, model architecture, and predictions\n",
    "    output_json = {\n",
    "        \"parameter_count\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "        \"model_architecture\": str(model),\n",
    "    }\n",
    "\n",
    "    # Save metadata JSON\n",
    "    with open(json_filename, \"w\") as f:\n",
    "        json.dump(output_json, f, indent=2)\n",
    "\n",
    "    # Download / display link depending on environment\n",
    "    if is_colab():\n",
    "        from google.colab import files\n",
    "        print(f\"OK: Saved as {json_filename}. Downloading in Colab...\")\n",
    "        files.download(json_filename)\n",
    "\n",
    "    elif is_kaggle():\n",
    "        from IPython.display import FileLink, display\n",
    "        print(\"#\" * 100)\n",
    "        print(f\"OK: Your submission file `{json_filename}` has been generated.\")\n",
    "        print(\"TODO: Click the link below.\")\n",
    "        print(\"1. The file will open in a new tab.\")\n",
    "        print(\"2. Right-click anywhere in the new tab and select 'Save As...'\")\n",
    "        print(\"3. Save the file to your computer with the `.json` extension.\")\n",
    "        print(\"You MUST submit this file to Autolab if this is your best submission.\")\n",
    "        print(\"#\" * 100 + \"\\n\")\n",
    "        display(FileLink(json_filename))\n",
    "\n",
    "    else:\n",
    "        print(f\"OK: saved model data saved to: '{json_filename}'\")\n",
    "        print(\"REQUIRED to submit to Autolab if these are the best model weights.\")\n",
    "\n",
    "generate_model_submission_file(model)\n",
    "#### IMPORTANT: Do NOT change the name of the model_metadata_....json file!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEPKqeYb5ec7"
   },
   "source": [
    "## TODO: fill in your submission requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9mbj34w5njD"
   },
   "source": [
    "### Notes:\n",
    "\n",
    "- You will need to set the root path to your submission files (eg. MODEL_METADATA_JSON, NOTEBOOK_PATH, HW4LIB_PATH). This will depend on your setup. For eg. if you are following our setup instruction:\n",
    "  - `Colab:`: `\"/content/...\"`In the left file pane, right-click the desired file or folder and select ‚ÄúCopy path‚Äù.\n",
    "  - `PSC`: `\"/jet/home/<your_username>/...\"` You can check the files in this path by running: ```!ls /jet/home/<your_username>/```\n",
    "\n",
    "Kindly modify your configurations to suit your ablations and be keen to include your name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Da4d7XOo5ez-"
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "#             README\n",
    "####################################\n",
    "\n",
    "# TODO: Please complete all components of this README\n",
    "README = \"\"\"\n",
    "- **Model**: Model archtiecture description. Anything unique? Any specific architecture shapes or strategies?\n",
    "- **Training Strategy**: optimizer + scheduler + loss function + any other unique ideas\n",
    "- **Augmentations**: augmentations if used. If augmentations weren't used, then ignore\n",
    "- **Notebook Execution**: Any instructions required to run your notebook.\n",
    "\"\"\"\n",
    "\n",
    "####################################\n",
    "#       Credentials (Optional)\n",
    "####################################\n",
    "\n",
    "# These are not required **IF** you have run the cells to declare these variables above.\n",
    "# If you would like to paste your credentials here again, feel free to:\n",
    "# OPTIONAL: Fill these out if you do not want to re-run previous cells to re-initialize these credential variables\n",
    "\n",
    "KAGGLE_USERNAME = \"todo-kaggle username\" #TODO\n",
    "KAGGLE_API_KEY = \"todo-kaggle key\" #TODO\n",
    "WANDB_API_KEY = \"todo-wandb key\" #TODO\n",
    "\n",
    "\n",
    "####################################\n",
    "#             Wandb Logs\n",
    "####################################\n",
    "\n",
    "# TODO: Your wandb project url should look like https://wandb.ai/username-or-team-name/project-name\n",
    "#(Take these parameters and put them in the variables below)\n",
    "\n",
    "WANDB_USERNAME_OR_TEAMNAME = \"todo-wandb username/teamname\" # TODO: Put your username-or-team-name here\n",
    "WANDB_PROJECT = \"todo-wandb project name\" # TODO: Put your project-name\n",
    "\n",
    "####################################\n",
    "#         Notebook & Files\n",
    "####################################\n",
    "\n",
    "# TODO: Download HW4P2 Notebook (if on colab or kaggle) and upload both your HW4P2 notebook + model_metadata_*.json to your file system.\n",
    "# TODO: For each file, obtain the file paths and put them below.\n",
    "\n",
    "# TODO: COLAB INSTRUCTIONS:\n",
    "# * With Colab, upload your desired file (notebook or model_metadata.json) to \"Files\"\n",
    "# * Right-click the file, click \"Copy Path,\"\n",
    "# * Paste the path below.\n",
    "\n",
    "# TODO: KAGGLE INSTRUCTIONS:\n",
    "# * First download a copy of your notebook with \"File > Download Notebook\"\n",
    "# Then...\n",
    "# * Click \"File\" in the top left of the screen\n",
    "# * Go to \"Upload Input > Upload Model\"\n",
    "# * Upload your notebook file.\n",
    "# * For \"Model Name\" put HW4P2_Final_Submission\n",
    "# * For \"Framework\" put \"Other\"\n",
    "# * For \"License\" put \"Other\"\n",
    "# * Click \"Upload another file\" and upload your model_metadata####.json file as well.\n",
    "# * Now, on your right in your \"Models\" section, you should see a new folder with your submission files.\n",
    "# * Click on the \"Copy File Path\" buttons for the notebook and json file and paste them below.\n",
    "\n",
    "# TODO: Linux system:\n",
    "# * Simply upload or find the path of your notebook file and model_metadata###.json file, and paste them here.\n",
    "\n",
    "NOTEBOOK_PATH = \"/content/drive/MyDrive/hw4p2/HW4P2_Student_Starter_Notebook.ipynb\" # TODO: Put your HW4P2 notebook path here\n",
    "MODEL_METADATA_JSON = \"/content/model_metadata_2025-07-14_21-42.json\" # TODO: Put your Model Metadata path json file here (see end of HW4P2 Code Notebook to get this file)\n",
    "HW4LIB_PATH = \"/content/hw4lib\" # TODO: Put your hw4lib path here\n",
    "\n",
    "####################################\n",
    "#         Additional Files\n",
    "####################################\n",
    "\n",
    "ADDITIONAL_FILES = [ # TODO: Upload any files and add any paths to any additional files you would like to include in your submission, otherwise, leave this empty\n",
    "]\n",
    "\n",
    "####################################\n",
    "#         SLACK SUBMISSION\n",
    "####################################\n",
    "\n",
    "ENABLE_SLACK_SUBMISSION = False # TODO: Set this to true if you are submitting to the Slack competition\n",
    "\n",
    "####################################\n",
    "#     Creating the Submission\n",
    "####################################\n",
    "\n",
    "# TODO: Once the README, wandb information, and file paths are filled in, run this cell,\n",
    "# run the \"Assignment Backend Functions\" in the next cells, and generate the final zip file at the end.\n",
    "\n",
    "SAFE_SUBMISSION = True # TODO: Set this to False if you want to generate a submission.zip even if you are missing files, otherwise it's recommended to keep this as True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdsePi6c5tj2"
   },
   "source": [
    "# Assignment Backend Submission Functions (DO NOT MODIFY, just run these cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMc7MjaK5jtb"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "######################################\n",
    "#       Assignment Configs\n",
    "######################################\n",
    "\n",
    "WANDB_METRIC = \"CER\"\n",
    "WANDB_DIRECTION = \"descending\"\n",
    "WANDB_TOP_N = 10\n",
    "WANDB_OUTPUT_PKL = \"wandb_top_runs.pkl\"\n",
    "\n",
    "# Kaggle configuration\n",
    "COMPETITION_NAME = \"11-785-hw-4-p-2-automatic-speech-recognition-f-25\"\n",
    "SLACK_COMPETITION_NAME = \"slack-hw-4-p-2-f-25\"\n",
    "FINAL_SUBMISSION_DATETIME = datetime.strptime(\"2025-12-06 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "SLACK_SUBMISSION_DATETIME = datetime.strptime(\"2025-12-11 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "GRADING_DIRECTION = \"descending\"\n",
    "KAGGLE_OUTPUT_JSON = \"kaggle_data.json\"\n",
    "\n",
    "SUBMISSION_OUTPUT = \"HW4P2_final_submission.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJ1k6Cs3jLmm"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import zoneinfo\n",
    "\n",
    "eastern = zoneinfo.ZoneInfo(\"America/New_York\")\n",
    "FINAL_DEADLINE_UTC = (\n",
    "    FINAL_SUBMISSION_DATETIME\n",
    "    .replace(tzinfo=eastern)\n",
    "    .astimezone(timezone.utc)\n",
    ")\n",
    "\n",
    "SLACK_DEADLINE_UTC = (\n",
    "    SLACK_SUBMISSION_DATETIME\n",
    "    .replace(tzinfo=eastern)\n",
    "    .astimezone(timezone.utc)\n",
    ")\n",
    "\n",
    "ACKNOWLEDGEMENT_MESSAGE = \"\"\"\n",
    "Submission of this file and assignment indicate the student's agreement to the following Aknowledgement requirements:\n",
    "Setting the ACNKOWLEDGED flag to True indicates full understanding and acceptance of the following:\n",
    "1. Slack days may ONLY be used on P2 FINAL (not checkpoint) submission. I.e. you may use slack days to submit final P2 kaggle scores (such as this one) later on the **SLACK KAGGLE COMPETITION** at the expense of your Slack days.\n",
    "2. The final autolab **code submission is due 48 hours after** the conclusion of the Kaggle Deadline (or, the same day as your final kaggle submission).\n",
    "3. Course staff will require your kaggle username here, and then will pull your official PRIVATE kaggle leaderboard score. This submission may result in slight variance in scores/code, but we will check for acceptable discrepancies. Any discrepancies related to modifying the submission code (at the bottom of the notebook) will result in an AIV.\n",
    "4. You are NOT allowed to use any code that will pre-load models (such as those from Hugging Face, etc.).\n",
    "   You MAY use models described by papers or articles, but you MUST implement them yourself through fundamental PyTorch operations (i.e. Linear, Conv2d, etc.).\n",
    "5. You are NOT allowed to use any external data/datasets at ANY point of this assignment.\n",
    "6. You may work with teammates to run ablations/experiments, BUT you must submit your OWN code and your OWN results.\n",
    "7. Failure to comply with the prior rules will be considered an Academic Integrity Violation (AIV).\n",
    "8. Late submissions MUST be submitted through the Slack Kaggle (see writeup for details). Any submissions made to the regular Kaggle after the original deadline will NOT be considered, no matter how many slack days remain for the student.\n",
    "\"\"\"\n",
    "def save_acknowledgment_file():\n",
    "    if ACKNOWLEDGED:\n",
    "        with open(\"acknowledgement.txt\", \"w\") as f:\n",
    "            f.write(ACKNOWLEDGEMENT_MESSAGE.strip())\n",
    "        print(\"Saved acknowledgement.txt\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"ERROR: Must set ACKNOWLEDGED = True.\")\n",
    "        return False\n",
    "# Saves README\n",
    "def save_readme(readme):\n",
    "    try:\n",
    "        with open(\"README.txt\", \"w\") as f:\n",
    "            f.write(readme.strip())\n",
    "\n",
    "        print(\"Saved README.txt\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error occured while saving README.txt: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Saves wandb logs\n",
    "import wandb, json, pickle\n",
    "\n",
    "def save_top_wandb_runs():\n",
    "    wandb.login(key=WANDB_API_KEY)\n",
    "    if not ACKNOWLEDGED:\n",
    "        print(\"ERROR: Must set ACKNOWLEDGED = True.\")\n",
    "        return False\n",
    "\n",
    "    api = wandb.Api()\n",
    "    runs = api.runs(\n",
    "        f\"{WANDB_USERNAME_OR_TEAMNAME}/{WANDB_PROJECT}\",\n",
    "        order=f\"{'-' if WANDB_DIRECTION == 'descending' else ''}summary_metrics.{WANDB_METRIC}\"\n",
    "    )\n",
    "    selected_runs = runs[:min(WANDB_TOP_N, len(runs))]\n",
    "\n",
    "    if not selected_runs:\n",
    "        print(f\"ERROR: No runs found for {WANDB_USERNAME_OR_TEAMNAME}/{WANDB_PROJECT}. Please check that your wandb credentials (Wandb Username/Team Name, API Key, and Project Name) are correct.\")\n",
    "        return False\n",
    "\n",
    "    all_data = []\n",
    "    for run in selected_runs:\n",
    "        run_data = {\n",
    "            \"id\": run.id,\n",
    "            \"name\": run.name,\n",
    "            \"tags\": run.tags,\n",
    "            \"state\": run.state,\n",
    "            \"created_at\": str(run.created_at),\n",
    "            \"config\": run.config,\n",
    "            \"summary\": dict(run.summary),\n",
    "        }\n",
    "        try:\n",
    "            run_data[\"history\"] = run.history(samples=1000)\n",
    "        except Exception as e:\n",
    "            run_data[\"history\"] = f\"Failed to fetch history: {str(e)}\"\n",
    "        all_data.append(run_data)\n",
    "    with open(WANDB_OUTPUT_PKL, \"wb\") as f:\n",
    "        pickle.dump(all_data, f)\n",
    "\n",
    "    print(f\"OK: Exported {len(all_data)} WandB runs to {WANDB_OUTPUT_PKL}\")\n",
    "\n",
    "    return True\n",
    "# Saves kaggle information\n",
    "\n",
    "# Install dependencies silently (only if running on Colab)\n",
    "import sys\n",
    "\n",
    "from datetime import datetime\n",
    "import os, json, requests\n",
    "def kaggle_login(username, key):\n",
    "    os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "    with open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"w\") as f:\n",
    "        json.dump({\"username\": username, \"key\": key}, f)\n",
    "    os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n",
    "\n",
    "\n",
    "def get_active_submission_config():\n",
    "    if ENABLE_SLACK_SUBMISSION:\n",
    "        return SLACK_COMPETITION_NAME, SLACK_DEADLINE_UTC\n",
    "    return COMPETITION_NAME, FINAL_DEADLINE_UTC\n",
    "\n",
    "def kaggle_user_exists(usernagbme):\n",
    "    try:\n",
    "        return requests.get(f\"https://www.kaggle.com/{KAGGLE_USERNAME}\").status_code == 200\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error occured while checking Kaggle user: {e}\")\n",
    "        return False\n",
    "\n",
    "DEFAULT_SCORE=0\n",
    "if GRADING_DIRECTION == \"ascending\":\n",
    "    DEFAULT_SCORE=0\n",
    "else:\n",
    "    DEFAULT_SCORE=1.0\n",
    "\n",
    "def get_best_kaggle_score(subs):\n",
    "    def extract_score(s): return float(s.private_score or s.public_score or DEFAULT_SCORE)\n",
    "    if not subs:\n",
    "        return None, None\n",
    "    best = max(subs, key=lambda s: extract_score(s) if GRADING_DIRECTION == \"ascending\" else -extract_score(s))\n",
    "\n",
    "    score_type = \"private\" if best.private_score not in [None, \"\"] else \"public\"\n",
    "    return extract_score(best), score_type\n",
    "\n",
    "def save_kaggle_json(kaggle_username, kaggle_key):\n",
    "\n",
    "    kaggle_login(kaggle_username, kaggle_key)\n",
    "\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    if not ACKNOWLEDGED:\n",
    "        print(\"ERROR: Must set ACKNOWLEDGED = True.\")\n",
    "        return False\n",
    "\n",
    "    if not kaggle_user_exists(KAGGLE_USERNAME):\n",
    "        print(f\"ERROR: User '{KAGGLE_USERNAME}' not found.\")\n",
    "        return False\n",
    "\n",
    "    comp_name, deadline = get_active_submission_config()\n",
    "\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "\n",
    "    # Get competition submissions\n",
    "    submissions = [s for s in api.competition_submissions(comp_name) if getattr(s, \"_submitted_by\", None) == KAGGLE_USERNAME]\n",
    "    if not submissions:\n",
    "        print(f\"ERROR: No valid submissions found for user [{KAGGLE_USERNAME}] for this competition [{comp_name}]. Slack flag set to [{ENABLE_SLACK_SUBMISSION}]\")\n",
    "        print(\"Please double check your Kaggle username and ensure you've submitted at least once.\")\n",
    "        return False\n",
    "\n",
    "    score, score_type = get_best_kaggle_score(submissions)\n",
    "    result = {\n",
    "        \"kaggle_username\": KAGGLE_USERNAME,\n",
    "        \"acknowledgement\": ACKNOWLEDGED,\n",
    "        \"submitted_slack\": ENABLE_SLACK_SUBMISSION,\n",
    "        \"competition_name\": comp_name,\n",
    "        \"deadline\": deadline.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"raw_score\": score * 100.0,\n",
    "        \"score_type\": score_type,\n",
    "    }\n",
    "\n",
    "    print(f\"OK: Projected score (excluding bonuses) saved as {KAGGLE_OUTPUT_JSON}\")\n",
    "    if score:\n",
    "        print(f\"Best score {score}.\")\n",
    "        with open(KAGGLE_OUTPUT_JSON, \"w\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "\n",
    "def create_submission_zip(additional_files, safe_flag):\n",
    "    if not \"ACKNOWLEDGED\" in globals() or not ACKNOWLEDGED:\n",
    "        print(\"ERROR: Make sure to RUN the Acknowledgement cell (at the top of the notebook). Also, must set ACKNOWLEDGED = True.\")\n",
    "        return\n",
    "\n",
    "    if (not save_acknowledgment_file()):\n",
    "        print(\"ERROR: Make sure to RUN the Acknowledgement cell (at the top of the notebook). Also, must set ACKNOWLEDGED = True.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    if not \"ENABLE_SLACK_SUBMISSION\" in globals() or ENABLE_SLACK_SUBMISSION is None:\n",
    "        print(\"ERROR: \\\"ENABLE_SLACK_SUBMISSION\\\" variable is not defined. \\nTODO: Make sure to RUN the cell (A few cells up at the beginning of the submission section). \\nMake sure to set the ENABLE_SLACK_SUBMISSION checkbox if you're on colab, or set the parameter correctly set on other platforms \\n(if you are submitting through the SLACK submission).\")\n",
    "        return\n",
    "\n",
    "    if not \"README\" in globals() or not README:\n",
    "        print(\"ERROR: Make sure to RUN the README cell(above your credentials cell).\")\n",
    "        return\n",
    "\n",
    "    if (not save_readme(README)):\n",
    "        print(\"ERROR: Error while saving the README file. Make sure to complete and RUN the README cell(above your credentials cell).\")\n",
    "        return\n",
    "\n",
    "    if (not save_top_wandb_runs()):\n",
    "        return\n",
    "\n",
    "    if not \"KAGGLE_USERNAME\" in globals() or not \"KAGGLE_API_KEY\" in globals() or not KAGGLE_USERNAME or not KAGGLE_API_KEY:\n",
    "        print(\"ERROR: Make sure to set KAGGLE_USERNAME and KAGGLE_API_KEY for this code submission.\")\n",
    "        return\n",
    "\n",
    "    if (not save_kaggle_json(KAGGLE_USERNAME, KAGGLE_API_KEY)):\n",
    "        print(f\"ERROR: An error occured while retrieve kaggle information from username [{KAGGLE_USERNAME}] from competition [{get_active_submission_config()[0]}] with slack flag set to [{ENABLE_SLACK_SUBMISSION}]. Please check your kaggle username, key, and submission.\")\n",
    "        return\n",
    "\n",
    "    files_to_zip = [\n",
    "        \"acknowledgement.txt\",\n",
    "        \"README.txt\",\n",
    "        KAGGLE_OUTPUT_JSON,\n",
    "        WANDB_OUTPUT_PKL,\n",
    "        MODEL_METADATA_JSON,\n",
    "        NOTEBOOK_PATH,\n",
    "        HW4LIB_PATH,\n",
    "    ] + additional_files\n",
    "\n",
    "    missing_files = False\n",
    "\n",
    "    with zipfile.ZipFile(SUBMISSION_OUTPUT, \"w\") as zipf:\n",
    "        for file_path in files_to_zip:\n",
    "            if os.path.exists(file_path):\n",
    "                arcname = os.path.basename(file_path)  # flatten path\n",
    "                zipf.write(file_path, arcname=arcname)\n",
    "                print(f\"OK: Added {arcname}\")\n",
    "            else:\n",
    "                missing_files = True\n",
    "                print(f\"ERROR: Missing file: {file_path}\")\n",
    "\n",
    "    if missing_files:\n",
    "        if safe_flag:\n",
    "            raise \"ERROR: Missing files with safety flag set to True. Please upload any necessary files, ensure you have the correct paths and rerun all cells.\"\n",
    "        else:\n",
    "            print(\"WARNING: Missing files with safety flag set to False. Submission may be incomplete.\")\n",
    "\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import files\n",
    "        files.download(SUBMISSION_OUTPUT)\n",
    "\n",
    "    print(\"Final submission saved as:\", SUBMISSION_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJl3y2kVjPTx"
   },
   "source": [
    "# File Generation (TODO: Check file generation outputs for any errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ru8ih53QjR2j"
   },
   "source": [
    "### For Colab and PSC users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIpfRmtEjMSp"
   },
   "outputs": [],
   "source": [
    "create_submission_zip(ADDITIONAL_FILES, SAFE_SUBMISSION)\n",
    "\n",
    "#TODO: If the HW4P2_final_submission.zip file does not\n",
    "# automatically bring up a donwload pop-up\n",
    "# Then make sure to entire the files and\n",
    "#manually download the checkpoint_submission.json file."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "QdhQJTlgXWXk",
    "k5ey-Nd0XWXx"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "idl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
